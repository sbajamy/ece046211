{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzV9wsJ5pGhf"
   },
   "source": [
    "# <img src=\"https://img.icons8.com/bubbles/50/000000/mind-map.png\" style=\"height:50px;display:inline\"> ECE 046211 - Technion - Deep Learning\n",
    "---\n",
    "\n",
    "## HW1 - Optimization and Automatic Differentiation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq2c8X93pGhh"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/clouds/96/000000/keyboard.png\" style=\"height:50px;display:inline\"> Keyboard Shortcuts\n",
    "---\n",
    "* Run current cell: **Ctrl + Enter**\n",
    "* Run current cell and move to the next: **Shift + Enter**\n",
    "* Show lines in a code cell: **Esc + L**\n",
    "* View function documentation: **Shift + Tab** inside the parenthesis or `help(name_of_module)`\n",
    "* New cell below: **Esc + B**\n",
    "* Delete cell: **Esc + D, D** (two D's)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZZybn3NpGhh"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/information.png\" style=\"height:50px;display:inline\"> Students Information\n",
    "---\n",
    "* Fill in\n",
    "\n",
    "|Name     |Campus Email| ID  |\n",
    "|---------|--------------------------------|----------|\n",
    "|Student 1| student_1@campus.technion.ac.il| 123456789|\n",
    "|Student 2| student_2@campus.technion.ac.il| 987654321|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDK5zqhdpGhi"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/upload-to-cloud.png\" style=\"height:50px;display:inline\"> Submission Guidelines\n",
    "---\n",
    "* Maximal garde: 100.\n",
    "* Submission only in **pairs**. \n",
    "    * Please make sure you have registered your group in Moodle (there is a group creation component on the Moodle where you need to create your group and assign members).\n",
    "* **No handwritten submissions.** You can choose whether to answer in a Markdown cell in this notebook or attach a PDF with your answers.\n",
    "* <a style='color:red'> SAVE THE NOTEBOOKS WITH THE OUTPUT, CODE CELLS THAT WERE NOT RUN WILL NOT GET ANY POINTS! </a>\n",
    "* What you have to submit:\n",
    "    * If you have answered the questions in the notebook, you should submit this file only, with the name: `ece046211_hw1_id1_id2.ipynb`.\n",
    "    * If you answered the questionss in a different file you should submit a `.zip` file with the name `ece046211_hw1_id1_id2.zip` with content:\n",
    "        * `ece046211_hw1_id1_id2.ipynb` - the code tasks\n",
    "        * `ece046211_hw1_id1_id2.pdf` - answers to questions.\n",
    "    * No other file-types (`.py`, `.docx`...) will be accepted.\n",
    "* Submission on the course website (Moodle).\n",
    "* **Latex in Colab** - in some cases, Latex equations may no be rendered. To avoid this, make sure to not use *bullets* in your answers (\"* some text here with Latex equations\" -> \"some text here with Latex equations\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmSj_UufpGhi"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/64/000000/online.png\" style=\"height:50px;display:inline\"> Working Online and Locally\n",
    "---\n",
    "* You can choose your working environment:\n",
    "    1. `Jupyter Notebook`, **locally** with <a href=\"https://www.anaconda.com/download\">Anaconda</a> or **online** on <a href=\"https://colab.research.google.com/\">Google Colab</a>\n",
    "        * Colab also supports running code on GPU, so if you don't have one, Colab is the way to go. To enable GPU on Colab, in the menu: `Runtime`$\\rightarrow$ `Change Runtime Type` $\\rightarrow$`GPU`.\n",
    "    2. Python IDE such as <a href=\"https://www.jetbrains.com/pycharm/\">PyCharm</a> or <a href=\"https://code.visualstudio.com/\">Visual Studio Code</a>.\n",
    "        * Both allow editing and running Jupyter Notebooks.\n",
    "\n",
    "* Please refer to `Setting Up the Working Environment.pdf` on the Moodle or our GitHub (https://github.com/taldatech/ee046211-deep-learning) to help you get everything installed.\n",
    "* If you need any technical assistance, please go to our Piazza forum (`hw1` folder) and describe your problem (preferably with images)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlp1Fp4ppGhj"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
    "---\n",
    "\n",
    "* [Part 1 - Theory](#-Part-1---Theory)\n",
    "    * [Q1 - Convergence of Gradient Descent](#-Question-1---Convergence-of-Gradient-Descent)\n",
    "    * [Q2 - Optimization and Gradient Descent](#-Question-2---Optimization-and-Gradient-Descent)\n",
    "    * [Q3 - Efficient Differentiation](#-Question-3---Efficient-Differentiation)\n",
    "    * [Q4 - Autodiff](#-Question-4----Automatic-Differentiation)\n",
    "* [Part 2 - Code Assignments](#-Part-2---Code-Assignments)\n",
    "    * [Task 1 - The Beale Function](#-Task-1---The-Beale-Function)\n",
    "    * [Task 2 - Building an Optimizer - Adam](#-Task-2---Building-an-Optimizer---Adam)\n",
    "    * [Task 3 - PyTorch Autograd](#-Task-3---PyTorch-Autograd)\n",
    "    * [Task 4 - Low Rank Matrix Factorization](#-Task-4---Low-Rank-Matrix-Factorization)\n",
    "* [Credits](#-Credits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKtSiQX_pGhj"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/cute-clipart/64/000000/ball-point-pen.png\" style=\"height:50px;display:inline\"> Part 1 - Theory\n",
    "---\n",
    "* You can choose whether to answser these straight in the notebook (Markdown + Latex) or use another editor (Word, LyX, Latex, Overleaf...) and submit an additional PDF file, **but no handwritten submissions**.\n",
    "* You can attach additional figures (drawings, graphs,...) in a separate PDF file, just make sure to refer to them in your answers.\n",
    "\n",
    "* $\\large\\LaTeX$ <a href=\"https://kapeli.com/cheat_sheets/LaTeX_Math_Symbols.docset/Contents/Resources/Documents/index\">Cheat-Sheet</a> (to write equations)\n",
    "    * <a href=\"http://tug.ctan.org/info/latex-refsheet/LaTeX_RefSheet.pdf\">Another Cheat-Sheet</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsqSFZG1pGhj"
   },
   "source": [
    "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 1 - Convergence of Gradient Descent\n",
    "---\n",
    "Recall from the lecture notes:\n",
    "\n",
    "* **Definition**: A function $f$ is $\\beta$-smooth if: $$ \\forall w_1, w_2 \\in \\mathbb{R}^d: ||\\nabla f(w_1) - \\nabla f(w_2)|| \\leq \\beta ||w_1 -w_2|| $$\n",
    "* **Lemma**: If $f$ is $\\beta$-smooth then $$ f(w_1) -f(w_2) -\\nabla f(w_2)^T (w_1-w_2) \\leq \\frac{\\beta}{2} ||w_1-w_2||^2 $$\n",
    "\n",
    "Prove the lemma.\n",
    "\n",
    "Hints:\n",
    "* Represent $f$ as an integral: $f(x) − f(y) = \\int_0^1 \\nabla f(y + t(x-y))^T(x-y) dt $\n",
    "* Make use of Cauchy-Schwarz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCb_MNJepGhk"
   },
   "source": [
    "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 2 - Optimization and Gradient Descent\n",
    "---\n",
    "The function $f: \\mathbb{R}^d \\to \\mathbb{R}$ is infinitely continuously differentiable, and satisfies $\\min_{w \\in \\mathbb{R}^d} f(w)=f_{*}> -\\infty$.\n",
    "\n",
    "We wish to minimize this function using a version of Gradient Descent (GD) with step-size $\\eta$, where in each iteration the gradients are multiplied by matrix $A$ $$ (*)\\: w(t+1) = w(t) -\\eta A\\nabla f\\left(w(t) \\right).$$\n",
    "\n",
    "Matrix $A$ is symmetric and strictly positive (positive definite with strictly positive eigenvalues), i.e., $\\lambda_{min} \\triangleq \\lambda_{min}(A) >0$, and denote $\\lambda_{max} \\triangleq \\lambda_{max}(A)$.\n",
    "\n",
    "1. In section only assume that $f(w)=\\frac{1}{2}w^THw$, where $H$ is strictly positive (positive definite with strictly positive eigenvalues). Find/choose $A$ and $\\eta$ such that the algorithm $(*)$ converges in minimal number of steps. Why is that choice is infeasible when $d$ is large? What is a common applicable approximation?\n",
    "2. Prove that Gradient Flow (i.e., GD in the limit $\\eta \\to 0$): $$ \\dot{w}(t)=-A\\nabla f \\left(w(t ) \\right) $$ converges to a critical point for all $f$ and $A$ that satisfy the conditions in the given question.\n",
    "    * **Hint**: from the properties of eigenvalues it satifies that $\\forall v\\in\\mathbb{R}^d: \\lambda_{min}||v||^2\\leq v^TAv\\leq \\lambda_{max}||v||^2$.\n",
    "3. Given that the function $f$ is $\\beta \\text{-smooth}$, find a condition on the step-size $\\eta$ such that we get convergence to a critical point in algorithm $(*)$. Prove convergence under this condition.\n",
    "    * **Hint**: for a $\\beta \\text{-smooth}$ function, one can write: $$ f\\left(w(t+1)\\right) -  f\\left(w(t)\\right) \\leq \\left(w(t+1) -w(t) \\right)^T\\nabla f\\left(w(t)\\right) + \\frac{\\beta}{2} ||w(t+1) -w(t) ||^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 3 - Efficient Differentiation\n",
    "---\n",
    "We wish to optimize a loss function $\\mathcal{L}\\left(\\mathbf{w}\\right)$ for  $\\mathbf{w}\\in\\mathbb{R}^{d}$ using Gradient Descent (GD) with some step size schedule $\\eta_{t}$\n",
    "\\begin{equation}\n",
    "(1)\\:\\: \\forall t=1,2,..:\\mathbf{w}\\left(t\\right)=\\mathbf{w}\\left(t-1\\right)-\\eta_{t}\\nabla\\mathcal{L}\\left(\\mathbf{w}\\left(t-1\\right)\\right)\n",
    "\\end{equation}\n",
    "initialized from some $\\mathbf{w}\\left(0\\right)$.\n",
    "We would like to learn the best step size schedule using GD. **Hint**: throughout this question, you should use the *chain rule*.\n",
    "\n",
    "1. Suppose we can consider each $\\eta_{t}$ as a separate parameter for each $t$. We initialize this parameter with $\\eta_{0}$ and update $\\eta_{t-1}$ with a GD step on $\\mathcal{L}\\left(\\mathbf{w}\\left(t-1\\right)\\right)$\n",
    "\\begin{equation}\n",
    "(2)\\:\\:\\eta_{t}=\\eta_{t-1}-\\alpha_{t}\\frac{\\partial\\mathcal{L}\\left(\\mathbf{w}\\left(t-1\\right)\\right)}{\\partial\\eta_{t-1}}\n",
    "\\end{equation} for every step of eq. $(1)$, where $\\alpha_{t}$ is the another step size. Calculate $\\partial\\mathcal{L}\\left(\\mathbf{w}\\left(t-1\\right)\\right)/\\partial\\eta_{t-1}$ as a function of the loss gradients $\\nabla\\mathcal{L}\\left(\\mathbf{w}\\left(t-1\\right)\\right)$ and $\\nabla\\mathcal{L}\\left(\\mathbf{w}\\left(t-2\\right)\\right)$.\n",
    "2. Now suppose we want to similarly update $\\alpha_{t-1}$ using GD step on $\\mathcal{L}\\left(\\mathbf{w}\\left(t-1\\right)\\right)$ every step of eq. $(2)$ with update step $\\kappa_{t}$\n",
    "\\begin{equation}\n",
    "\\alpha_{t}=\\alpha_{t-1}-\\kappa_{t}\\frac{\\partial\\mathcal{L}\\left(\\mathbf{w}\\left(t-1\\right)\\right)}{\\partial\\alpha_{t-1}}\\,.\n",
    "\\end{equation} Calculate $\\partial\\mathcal{L}\\left(\\mathbf{w}\\left(t-1\\right)\\right)/\\partial\\alpha_{t-1}$ as a function of $\\left\\{ \\nabla\\mathcal{L}\\left(\\mathbf{w}\\left(t-k\\right)\\right)\\right\\} _{k=1}^{3}$.\n",
    "3. Now we wish to update $\\left(\\eta_{t-1},\\eta_{t-2}\\right)$ by doing a GD step on $\\mathcal{L}\\left(\\mathbf{w}\\left(t-1\\right)\\right)$\n",
    "\\begin{equation}\n",
    "(3)\\:\\:\\left(\\eta_{t+1},\\eta_{t}\\right)=\\left(\\eta_{t-1},\\eta_{t-2}\\right)-\\alpha_{t}\\left(\\frac{\\partial\\mathcal{L}\\left(\\mathbf{w}\\left(t-1\\right)\\right)}{\\partial\\eta_{t-1}},\\frac{\\partial\\mathcal{L}\\left(\\mathbf{w}\\left(t-1\\right)\\right)}{\\partial\\eta_{t-2}}\\right)\n",
    "\\end{equation} every two steps of eq. $(1)$. Calculate the derivative $\\frac{\\partial\\mathcal{L}\\left(\\mathbf{w}\\left(t-1\\right)\\right)}{\\partial\\eta_{t-2}}$  as a function of $\\eta_{t-1}$, $\\left\\{ \\nabla\\mathcal{L}\\left(\\mathbf{w}\\left(t-k\\right)\\right)\\right\\} _{k=1}^{3}$, and $\\nabla^{2}\\mathcal{L}\\left(\\mathbf{w}\\left(t-2\\right)\\right)$.\n",
    "4. Now we wish again to update $\\left(\\eta_{t},\\eta_{t+1},...,\\eta_{t+T}\\right)$ by doing a GD step on $\\mathcal{L}\\left(\\mathbf{w}\\left(t-1\\right)\\right)$ every $T$ steps of eq. $(1)$\n",
    "\\begin{equation}\n",
    "(4)\\:\\:\\left(\\eta_{t+T},...,\\eta_{t}\\right)=\\left(\\eta_{t-1},...,\\eta_{t-1-T}\\right)-\\alpha_{t}\\left(\\frac{\\partial\\mathcal{L}\\left(\\mathbf{w}\\left(t-1\\right)\\right)}{\\partial\\eta_{t-1}},...,\\frac{\\partial\\mathcal{L}\\left(\\mathbf{w}\\left(t-1\\right)\\right)}{\\partial\\eta_{t-1-T}}\\right)\n",
    "\\end{equation} Calculate the derivative $\\frac{\\partial\\mathcal{L}\\left(\\mathbf{w}\\left(t-1\\right)\\right)}{\\partial\\eta_{t-\\tau}}$ as a function of $\\left\\{ \\eta_{t-k},\\nabla^{2}\\mathcal{L}\\left(\\mathbf{w}\\left(t-k-1\\right)\\right)\\right\\} _{k=1}^{\\tau-1}$, $\\nabla\\mathcal{L}\\left(\\mathbf{w}\\left(t-1\\right)\\right)$ and $\\nabla\\mathcal{L}\\left(\\mathbf{w}\\left(t-\\tau-1\\right)\\right)$.\n",
    "5. Compare this approach (eq. $(4)$ with $T>1$) to the first one (eq. $(2)$). Name one advantage for each approach.\n",
    " Hints: Think of computional complexity, ease of optimization, suitability\n",
    " of the objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVF9OJWWpGhl"
   },
   "source": [
    "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 4 -  Automatic Differentiation\n",
    "---\n",
    "\n",
    "Consider the scalar function: $$ f = \\exp(\\exp(x) + \\exp(x)^2) +\\sin(\\exp(x) + \\exp(x)^2) $$\n",
    "\n",
    "1. Write down the derivative w.r.t. $x$ explicitly, i.e., $\\frac{d f}{d x}$\n",
    "2. We define the following intermediate variables: $$ a = \\exp(x) $$ $$ b=a^2 $$ $$ c = a+b $$ $$ d=\\exp(c) $$ $$ e=\\sin(c) $$ $$ f=d+e $$ Draw a graph picturing the relationship between all variables (called the **computation graph**).\n",
    "3. Using the graph, write down the derivatives of the individual terms, working backwards to compute the derivative of $f$ (i.e., write down the derivatives $\\frac{df}{dd}, \\frac{df}{de}, ..., \\frac{df}{dx}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7D-14iM7pGhm"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/officel/80/000000/code.png\" style=\"height:50px;display:inline\"> Part 2 - Code Assignments\n",
    "---\n",
    "* You must write your code in this notebook and save it with the output of aall of the code cells.\n",
    "* Additional text can be added in Markdown cells.\n",
    "* You can use any other IDE you like (PyCharm, VSCode...) to write/debug your code, but for the submission you must copy it to this notebook, run the code and save the notebook with the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bya0qUGYpGhm"
   },
   "outputs": [],
   "source": [
    "# imports for the practice (you can add more if you need)\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import LogNorm\n",
    "from sklearn.datasets import load_iris\n",
    "seed = 211\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "# %matplotlib notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HC-UE1cNpGhn"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 1 - The Beale Function\n",
    "---\n",
    "The Beale function is defined as follows: $$ f(x, y) = (1.5 - x + xy)^{2} + (2.25 - x + xy^{2})^{2} + (2.625 - x +xy^{3})^{2}$$\n",
    "\n",
    "1. What is the global minima of this function?\n",
    "2. Implement the Beale function: `beale_f(x,y)`.\n",
    "3. Implement a function, `beale_grads(x,y)` that returns the gradients of the Beale function.\n",
    "4. 3D plot the Beale function wit the global minima you found. Use Matplotlib's `ax.plot_surface(x_mesh, y_mesh, z, norm=LogNorm(), rstride=1, cstride=1, edgecolor='none', alpha=.8, cmap=plt.cm.jet)` for the function, and `ax.plot(x, y, f(x, y), 'r*', markersize=20)` for the minima.\n",
    "4. 2D plot the contours with `ax.contour(x_mesh, y_mesh, z, levels=np.logspace(-.5, 5, 35), norm=LogNorm(), cmap=plt.cm.jet)` and the minima with `ax.plot(x, y, 'r*', markersize=20)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsHtDWllpGhn"
   },
   "source": [
    "Your Answers Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ukJi9-rapGhn"
   },
   "outputs": [],
   "source": [
    "# Set the manually calculated minima\n",
    "min_x = None\n",
    "min_y = None\n",
    "\n",
    "def beale_f(x, y):\n",
    "    value = None\n",
    "    \"\"\"\n",
    "    Your Code Here\n",
    "    \"\"\"\n",
    "    return value\n",
    "\n",
    "def beale_grads(x, y):\n",
    "    dx, dy = None, None\n",
    "    \"\"\"\n",
    "    Your Code Here\n",
    "    \"\"\"\n",
    "\n",
    "    grads = np.array([dx, dy])\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IepBb4_9pGho"
   },
   "outputs": [],
   "source": [
    "minima = np.array([min_x, min_y])\n",
    "beale_res = beale_f(*minima)\n",
    "grads_res = beale_grads(*minima)\n",
    "print(f\"minima (1x2 row vector shape): {minima}\")\n",
    "print(f\"beale_f output: {beale_res}\")\n",
    "print(f\"beale_grad output: {grads_res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 2 - Building an Optimizer - Adam\n",
    "---\n",
    "In this task, you are going to implement the Adam optimizer. We are giving the skeleton of the code and the description of the methods, and you need to implement the optimizer.\n",
    "\n",
    "Recall the Adam update rule:\n",
    "$$ m_{k+1} = \\beta_1 m_k + (1-\\beta_1)\\nabla f(w^k) = \\beta_1 m_k + (1-\\beta_1)g_k $$  $$ v_{k+1} = \\beta_2 v_k + (1-\\beta_2)(\\nabla f(w^k))^2 = \\beta_2 v_k + (1-\\beta_2)g^2_k $$ Then, they use an **unbiased** estimation: $$ \\hat{m}_{k+1} = \\frac{m_{k+1}}{1 -\\beta_1^{k+1}} $$ $$ \\hat{v}_{k+1} = \\frac{v_{k+1}}{1 -\\beta_2^{k+1}} $$ (the $\\beta$'s are taken with the power of the current iteration) $$ w_{k+1} = w_k -\\frac{\\alpha}{\\sqrt{\\hat{v}_{k+1}} +\\epsilon}\\hat{m}_{k+1} $$\n",
    "\n",
    "* $\\epsilon$ deafult's is $10^{-8}$\n",
    "\n",
    "\n",
    "1. Implement `class AdamOptimizer()`. \n",
    "    * `function` is the Python function you want to optimize.\n",
    "    * `gradients` is the Python function that returns the gradients of `function`.\n",
    "    * `x_init` and `y_init` are the initialization points for the optimizer.\n",
    "    * Save the `path` of the optimizer (the minima points the optimizer visits during the optimization).\n",
    "    * Stopping criterion: change in minima `<1e-7`.\n",
    "    * **You can change the class however you wish, you can remove/add variables and methods as you wish**\n",
    "2. For ` x_init=0.7, y_init=1.4, learning_rate=0.1, beta1=0.9, beta2=0.999`, optimize the Beale function. Plot the results **with the path taken** (better do it on the 2D contour plot).\n",
    "3. Choose different initialization and learning rate and show the results as in 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer():\n",
    "    def __init__(self, function, gradients, x_init=None, y_init=None, \n",
    "                 learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.f = function\n",
    "        self.g = gradients\n",
    "        scale = 3.0\n",
    "        self.current_val = np.zeros([2])\n",
    "        if x_init is not None:\n",
    "            self.current_val[0] = x_init\n",
    "        else:\n",
    "            self.current_val[0] = np.random.uniform(low=-scale, high=scale)\n",
    "        if y_init is not None:\n",
    "            self.current_val[1] = y_init\n",
    "        else:\n",
    "            self.current_val[1] = np.random.uniform(low=-scale, high=scale)\n",
    "        print(\"x_init: {:.3f}\".format(self.current_val[0]))\n",
    "        print(\"y_init: {:.3f}\".format(self.current_val[1]))\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.grads_first_moment = np.zeros([2])\n",
    "        self.grads_second_moment = np.zeros([2])\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # for accumulation of loss and path (w, b)\n",
    "        self.z_history = []\n",
    "        self.x_history = []\n",
    "        self.y_history = []\n",
    "\n",
    "\n",
    "    def func(self, variables):\n",
    "        \"\"\"Beale function.\n",
    "\n",
    "        Args:\n",
    "          variables: input data, shape: 1-rank Tensor (vector) np.array\n",
    "            x: x-dimension of inputs\n",
    "            y: y-dimension of inputs\n",
    "\n",
    "        Returns:\n",
    "          z: Beale function value at (x, y)\n",
    "        \"\"\"\n",
    "\n",
    "    def gradients(self, variables):\n",
    "        \"\"\"Gradient of Beale function.\n",
    "\n",
    "        Args:\n",
    "          variables: input data, shape: 1-rank Tensor (vector) np.array\n",
    "            x: x-dimension of inputs\n",
    "            y: y-dimension of inputs\n",
    "\n",
    "        Returns:\n",
    "          grads: [dx, dy], shape: 1-rank Tensor (vector) np.array\n",
    "            dx: gradient of Beale function with respect to x-dimension of inputs\n",
    "            dy: gradient of Beale function with respect to y-dimension of inputs\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    def weights_update(self, grads, time):\n",
    "        \"\"\"Weights update using Adam.\n",
    "\n",
    "          g1 = beta1 * g1 + (1 - beta1) * grads\n",
    "          g2 = beta2 * g2 + (1 - beta2) * grads ** 2\n",
    "          g1_unbiased = g1 / (1 - beta1**time)\n",
    "          g2_unbiased = g2 / (1 - beta2**time)\n",
    "          w = w - lr * g1_unbiased / (sqrt(g2_unbiased) + epsilon)\n",
    "        \"\"\"\n",
    "        \n",
    "    def history_update(self, z, x, y):\n",
    "        \"\"\"Accumulate all interesting variables\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    def train(self, max_steps):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mKouk-J0pGhq"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your Code Here\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = AdamOptimizer(beale_f, beale_grads, x_init=0.7, y_init=1.4, learning_rate=0.1, beta1=0.9, beta2=0.999, epsilon=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time\n",
    "opt.train(1000)\n",
    "print(\"Global minima\")\n",
    "print(\"x*: {:.2f}  y*: {:.2f}\".format(minima[0], minima[1]))\n",
    "print(\"Solution using the gradient descent\")\n",
    "print(\"x: {:.4f}  y: {:.4f}\".format(opt.x, opt.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the Beale function values during the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the optimization path\n",
    "path = opt.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGSxoUKppGhs"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 3 - PyTorch Autograd\n",
    "---\n",
    "For the function from the theory practice: $$ f = \\exp(\\exp(x) + \\exp(x)^2) +\\sin(\\exp(x) + \\exp(x)^2)  $$\n",
    "\n",
    "1. Implement it and its dervative (explicitly) using `torch`.\n",
    "2. Define a scalar tensor `x` and use `autograd` to calculate the derivative w.r.t $x$. Does the result correspond to the output of the function the calculates the derivative explicitly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qopWh-QupGhs"
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    f_val = None\n",
    "    \"\"\"\n",
    "    Your Code Here\n",
    "    \"\"\"\n",
    "    return f_val\n",
    "\n",
    "def derv_f(x):\n",
    "    derv_val = None\n",
    "    \"\"\"\n",
    "    Your Code Here\n",
    "    \"\"\"\n",
    "    return derv_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oNO19wRspGhs"
   },
   "outputs": [],
   "source": [
    "x = torch.tensor(0.5, requires_grad=True)\n",
    "print(x)\n",
    "f_res = f(x)\n",
    "f_manual_grad = derv_f(x.detach()) \n",
    "\n",
    "\"\"\"\n",
    "Your Code Here\n",
    "\"\"\"\n",
    "# Calculate with torch autograd\n",
    "f_autograd = None\n",
    "\n",
    "\n",
    "print(f_manual_grad)\n",
    "print(f_autograd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jrof_SsJpGht"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 4 - Low Rank Matrix Factorization \n",
    "---\n",
    "Consider the following optimization problem: $$ \\min_{\\hat{U}, \\hat{V}}||A - \\hat{U}\\hat{V}||_F^{2} $$ Where $A \\in \\mathcal{R}^{m \\times n},\\hat{U} \\in \\mathcal{R}^{m \\times r}, \\hat{V} \\in \\mathcal{R}^{r \\times n} $ and $r < min(m,n)$ ($r$ is the rank of the matrix). $||\\cdot||_F^2$ denotes the Frobenius norm.\n",
    "\n",
    "1. Implement a function, `gd_factorize_ad(A, rank, num_epochs=1000, lr=0.01)`, that given a 2D tensor `A` and a `rank`, will calculate the low-rank factorization of `A` using **gradient decsent**. Compute and apply all the gradients of $\\hat{U}$ and of $\\hat{V}$ once per epoch. $\\hat{U}$ and $\\hat{V}$ should be initially created with uniform random values. Use PyTorch's `autograd` for the gradients.\n",
    "    * To compute the squared Frobenius norm loss (reconstruction loss), use `torch.nn.functional.mse loss with reduction=’sum’`.\n",
    "\n",
    "2. Use the provided `data` of the Iris dataset of 150 instances and 4 features. Apply `gd_factorize_ad` to compute the 2-rank matrix factorization of `data`. What is the reconstruction loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HCFYO3QBpGht"
   },
   "outputs": [],
   "source": [
    "df = load_iris(as_frame=True).data # option 1\n",
    "# df = pd.read_csv('./iris.data', header=None) # option 2\n",
    "data = torch.tensor(df.iloc[:, [0, 1, 2, 3]].values)\n",
    "data = data - data.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sLIgji02pGht"
   },
   "outputs": [],
   "source": [
    "def gd_factorize_ad(A, rank, num_epochs=1000, lr=0.01):\n",
    "    # initialize\n",
    "    U = None\n",
    "    V = None\n",
    "    \n",
    "    \"\"\"\n",
    "    Your Code Here\n",
    "    \"\"\"\n",
    "    \n",
    "    # implement gradient descent\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        \"\"\"\n",
    "        Your Code Here\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if epoch % 5 == 0:\n",
    "            print(f'epoch: {epoch}, loss: {loss}')\n",
    "    return U, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZmImbLvIpGht"
   },
   "outputs": [],
   "source": [
    "U, V = gd_factorize_ad(data.float(), rank=2, num_epochs=1000, lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZgZUKV2NpGhu"
   },
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
    "---\n",
    "* Icons made by <a href=\"https://www.flaticon.com/authors/becris\" title=\"Becris\">Becris</a> from <a href=\"https://www.flaticon.com/\" title=\"Flaticon\">www.flaticon.com</a>\n",
    "* Icons from <a href=\"https://icons8.com/\">Icons8.com</a> - https://icons8.com\n",
    "* Datasets from <a href=\"https://www.kaggle.com/\">Kaggle</a> - https://www.kaggle.com/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "ee046211_hw1_optimization_autograd_sol.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
